
**ðŸ“ Take-Home Task Final Report**

!! Following report is generated by gemini with feeding ``backlog-timeline`` file as input. Also totally 4 days have been reserved to this take-home assignment.

- Following is an overall summary on the architectural updates. After I got fal serverless credits I applied most of them to fal serverless as well. Only tensorrt version of the model could not be utilized because of the architectural differences between my local GPU and serverless gpu's. 

## **ðŸš€ Executive Summary**

This report summarizes the progress and optimizations achieved while implementing and enhancing a Text-to-Image (T2I) and Image-to-Image (I2I) generation pipeline based on the **Wan2.2** architecture. The initial focus was on adapting the video generation model to a single-image context, followed by significant efforts in **performance optimization** and the implementation of **additional functionality** beyond the core requirements.

Key achievements include:

* **Core T2I/I2I Implementation:** Successfully adapted the Wan2.2 (Video Generation) architecture to perform high-quality T2I and I2I tasks.  
* **Performance Optimization (Up to x10 Speedup in Text Encoding):** Achieved substantial speed gains using **NVIDIA TensorRT (trtexec)** for model conversion and utilizing **DeepCache** for accelerated denoising steps.  
* **New Functionality:** Implemented a **Multiple Image Blending** feature and refined the **"Creativity"** hyperparameter for I2I control.

## **âœ… Core Implementation (Wan2.2 Adaptation)**

The initial phase involved adapting the Wan2.2 video generation model to handle single-frame image generation.

| Date | Focus | Description & Findings |
| :---- | :---- | :---- |
| 3.12.2025 | Architecture Analysis | Noticed the model uses an MLP block for integrating **timeframe information** relevant for video. Decided to isolate the single-image generation process by recording and using the output for the **0th timeframe** as input to the relevant transformer blocks. |
| 4.12.2025 | T2I/I2I Integration | Successfully integrated T2I and I2I pipelines. |
| 4.12.2025 | "Creativity" Parameter | Introduced a tunable hyperparameter, **"Creativity"** in range \[0, 1\] , to control the blend ratio of image latent and noise, thus regulating how different the new image will be from the input. This value determines the amount noise(ratio we start denoising) added to the input image.  |

* **Relevant Files:** examples/alper-4\_12\_2025 and examples/alper-4\_12\_2025\_v2

## **âš¡ Performance Optimization**

The primary goal was to drastically reduce inference time, particularly in the most time-consuming components.

### **1\. T5 Text Encoder Optimization**

| Date | Method | Speedup/Results | Relevant Files |
| :---- | :---- | :---- | :---- |
| 6.12.2025 | torch.compile | Reduced T5 inference time from 6 seconds (for 2 inferences) to 2 seconds (**x3 speedup**). | without\_opt.json, process\_timer\_output.ipynb |
| 9.12.2025 (6 PM) | **TensorRT (trtexec)** | Converted T5 to ONNX/Engine format. Achieved a single inference time of 0.009 seconds, down from  0.1 seconds (**x10 speedup**). | examples/alper\_9\_12\_2025\_t5\_opt\_v2\_engine |
| 9.12.2025 (12 AM) | **Negative Prompt Caching** | Cached the Text Encoder output for the negative prompt (as it is rarely updated), resulting in an additional **x2 speedup** in the overall text encoding phase. | examples/alper\_9\_12\_2025\_t5neg\_prompt\_caching |

### **2\. DeepCache Integration (Diffusion Process)**

DeepCache was used to accelerate the diffusion process by skipping calculations in early layers where high-level features change slowly.

| Date | DeepCache Parameters | Resulting Performance & Quality |
| :---- | :---- | :---- |
| 9.12.2025 (1 AM) | **Baseline (No DeepCache)** | Forward step: 0.18 seconds. |
|  | interval=2, block\_num=20 | Average Step Time: 0.11 seconds (~ 40% speed gain). **Visible quality degradation.** |
|  | **Optimal: interval=2, block\_num=10** | Average Step Time:  0.14 seconds ( 22\% speed gain). **High visual quality maintained.** |
|  | interval=3, 4 | Maintained valid outputs with speed gains of  27\% and  33\%, respectively. |

* **Relevant Files:** examples/alper\_9\_12\_2025\_deepcache

## **âœ¨ Additional Functionality**

The following new feature was implemented to expand the utility of the core pipeline.

### **Multiple Image Blending (Latent Mixing)**

| Date | Feature Description | Results |
| :---- | :---- | :---- |
| 6.12.2025 | Implemented the ability to blend more than one input image to generate a new composite image. This was achieved by averaging the **VAE-encoded latents** of the input images, exploiting the continuous nature of the latent space. Text guidance remains active to steer the final result. | Successfully generated blended images that represent both inputs while following the text prompt. |

* **Relevant Files:** examples/alper\_6\_12\_2025\_latent\_mixing

## **ðŸš§ Sampler/Scheduler Exploration**

Exploration into optimizing the number of denoising steps by using alternative samplers was conducted but did not yield immediate, substantial results in the initial 6 steps with the currently available options. Further research and experimentation are needed in this area.



## **Final Remarks**

I also have following ideas to further improve the speed but due to limited time I was not able to explore deeply:

* **Engine generation for DiT and VAE models:** The pipeline of pytorch->onnx->tensorrt can be applied to DiT and VAE models as well. Due to optimization in computation graphs, this will lead to very high amount of speed gain
* **LCM LoRA Training (Distillation):** Upon my research I have noticed that LCM's(Latent Consistency Model) are go to options when the target is to speed up diffusion process. While LoRA can be used to inject specific style to diffusion process, it can be used for modeling LCM's as well. With conditioning model to predict the total loss instead of loss at the given step, one can reduce the number of iterations to 4-8. I have used Wan2.2 A5B model in my take-home. I tried to find if there are any existing LCM LoRA for this model but I only could find style LoRA's. I am curious about how it would perform
* **Using Different Samplers:** One can plug and play different samplers when doing diffusion. While some of the solvers are straightforward and fast, others solve higher order ODE/SDE' s and can be more complex. So there is a trade-off on selection of the solvers. I wanted to plug and play with different solvers and see if I can get a comparably well results with fewer steps. But unfortunately I was not successfull. I believe I was not able to integrate the correct noise scaling into my custom sovlers but because of the limited time I proceeded

Trying following features to see their outputs would also be fun:

* **Integrating sound input to generation:** Probably not very common but wan2.2 already has i2v pipeline which we can submit sound input. I would love to integrate it to i2i pipe and see if it can generate birds when I gave bird sounds to it
* **Style LoRA's:** As I mentioned earlier, there is existing LoRA models for Wan2.2 A5B. One is very fun where input images are compressed with a hydraulic press machine [link](https://huggingface.co/ostris/wan22_5b_i2v_crush_it_lora). Would love to train one and try out if I can generate anime style iamges. 
