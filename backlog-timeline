Logs 

- 9.12.2025 1AM

DeepCache is a method that allows us to eliminate a number of operations during denoising steps. It is observed that, high-level features change very slowly between close steps. And in the early layers of encoding input data ( First N layers of DiT ) high level semantic data is calculated. The latents calculated in these early steps are very similar across timesteps. Thus we can skip these early layers in certain periods. Actually there are two parameters we can play with: CAHCE_INTERVAL, CACHE_BLOCKS. We cache every CAHCE_INTERVAL, the first  CACHE_BLOCKS block output. DiT in Wan architecture has 32 layers and I was able to experiment different with different number of blocks as well as different interval values. Following is a short summary of my findings: 

Without deepcache a forward step of Wan model takes ~0.18 seconds

With interval 2 and block_num 20, the inference time is reduced with a very well magnitude: (0.18+0.03)/2=~0.11 Which corresponds to %40 speed increase for the diffusion process. However, with this methodology, we also lose fine details and there is a visible quality degragaiton in the generated image.

With interval 2 and block_num 10, the optimized step takes ~0.093. The average is ~0.14 and overall speed gain is %22. Also the image is visually high quality.

We can also increase the interval to 3, 4 which correspond to %27, ~%33 and outputs still look valid. We can try to find the best parameters with trial. The paper uses higher interval values and less percentage of the blocks are kept. 

You can take a look at the visual results at examples/alper_9_12_2025_deepcache as well as the timer outputs



- 6.12.2025 9PM


I have tried many samplers from diffuser library however I was not able to successfully integrate any of them into the current pipeline. Theoritically I should be able to plug in any sampler, and upon my research we have sampler options that might give us better results for lower number of iterations. I have conducted experiments with existing two of the samplers that are developed by wan engineers but in 6 steps I was not able to notice any substantial difference between generations in terms of accuracy. Therefore reducing the number of iterations with a better sampler is not an available option for now. 

I decided that I have already spent too much time on samplers, thus I wanted to move on with the other part of the assignment "Additional functionality"

There are many options that we can add to our existing pipeline but first feature that I wanted to implement is multiple image blending. This feature allows us to upload more than one image and create an iamge out of these. Text guidance is still relevant but we only update the image latent. Because VAE' s produce continous latent space's, we can easily take average of more than one latent and get a valid latent that represents both of the inputs. You can observe the example outputs from the folder examples/alper_6_12_2025_latent_mixing. Where inputs are fb.jpg and input.jpg, the text input is Boxer cat wears the red gloves. Has an angry look and threathens his opponents.

- 6.12.2025 4PM

I created a class named "Timer" to measure how much of time each component of the model take. After initial observation, it looked like T5 text model was taking too much of the time compared to other parts. We can directly optimized this part via torch.compile. And even with defualt settings I was able to push the model speed 3x. From ~6 seconds for 2 t5 inference to ~2 seconds. 

You can also look at the time usage of other parts from without_opt.json file and the jupyter notebook where I try t5 optimization in an isolated environment at process_timer_output.ipynb.

I could do the same optimization with both VAE and DiT blocks but in my current capabilities with my graphics card 3090, I am already pushing it to it's limit when optimazing. I am planning on moving forward with other types of optimizations such as using different sampler/schedulers


- 4.12.2025 10:25PM


I noticed that i2i generations are not as good looking as the t2i pipeline which I later figured out which is actually a big flaw. The new system is as follows: We get the latent from encoder. We also get timesteps from the scheduler. We decide on a  percentage of available steps that we want to go over which essentially allows us to decide the randomness level from the image guidance perspective. We get a parameter named "creativity" which takes values in range [0,1]. You can find example images of it at examples/alper-4_12_2025_v2


- 4.12.2025 4PM

I was able to integrate i2i and t2i pipelines from ti2v task. For t2i task was actually trivial because I only updated # of frames to be generated and save the resulting 
tensor as image which util was already implemented

i2i task was a little bit tricky because ti2v pipeline starts from the given image and gradually get a scene out of it. However in order to generate a single image, the idea of generating a series of images to wait that kind
of variance does not make sense. When I looked at the code and look at the paper, I see that latenst(image) are initialized as random in t2v pipeline and with the image encoding in i2v pipeline. We need something in the middle in order to 
generate a different image compared to given image. 

When I look at the code they use masks to either mask out the image latent or random noise. Because we need to generate a latent which is not too far away from the given image's latent but still be close to it
I decided to use mask values which range between 0-1 rather than using fixed 1's or 0's. And set it as a hyperparameter named "creativity". This way we can tune this parameter to decide on how different we want new image to be from the given.

I have given examples in the folder: examples/alper-4_12_2025

!! Observation: The generated image in the i2i pipeline looks suboptimal with visible texture problems. Is this related to wrong configuration of latent variable update ? I' ll be looking into this in more detail.






- 3.12.2025 11PM

I have skimmed trough the paper. It seems that they have pretrained the model first with text-to-image dataset ensuring the cross-model semantic-textual alignment. 
This gives a huge hint about text-to-image implementation. 

Also when I look at the model architecture, I see an MLP block used for integrating timestep information. We do not need this block since we will not be generating videos. We can record the outputs for 0th timestep and use that value as input to the relevant transformer blocks

- 3.12.2025

I got the description of implementing Wan2.2 t2i and i2i. However, Wan2.2 is a video generation model. Using wan2.2 api to generate a single frame video is trivial which should be bare minimum(implementing requirements for serverless). 
I believe going over the architecture of Wan2.2 I can isolate the process of generating single image from the existing functionality at the model. Therefore I start working by reading the paper of Wan2.2

