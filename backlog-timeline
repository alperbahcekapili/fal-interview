Logs 

- 6.12.2025 9PM


I have tried many samplers from diffuser library however I was not able to successfully integrate any of them into the current pipeline. Theoritically I should be able to plug in any sampler, and upon my research we have sampler options that might give us better results for lower number of iterations. I have conducted experiments with existing two of the samplers that are developed by wan engineers but in 6 steps I was not able to notice any substantial difference between generations in terms of accuracy. Therefore reducing the number of iterations with a better sampler is not an available option for now. 

I decided that I have already spent too much time on samplers, thus I wanted to move on with the other part of the assignment "Additional functionality"

There are many options that we can add to our existing pipeline but first feature that I wanted to implement is multiple image blending. This feature allows us to upload more than one image and create an iamge out of these. Text guidance is still relevant but we only update the image latent. Because VAE' s produce continous latent space's, we can easily take average of more than one latent and get a valid latent that represents both of the inputs. You can observe the example outputs from the folder examples/alper_6_12_2025_latent_mixing. Where inputs are fb.jpg and input.jpg, the text input is Boxer cat wears the red gloves. Has an angry look and threathens his opponents.

- 6.12.2025 4PM

I created a class named "Timer" to measure how much of time each component of the model take. After initial observation, it looked like T5 text model was taking too much of the time compared to other parts. We can directly optimized this part via torch.compile. And even with defualt settings I was able to push the model speed 3x. From ~6 seconds for 2 t5 inference to ~2 seconds. 

You can also look at the time usage of other parts from without_opt.json file and the jupyter notebook where I try t5 optimization in an isolated environment at process_timer_output.ipynb.

I could do the same optimization with both VAE and DiT blocks but in my current capabilities with my graphics card 3090, I am already pushing it to it's limit when optimazing. I am planning on moving forward with other types of optimizations such as using different sampler/schedulers


- 4.12.2025 10:25PM


I noticed that i2i generations are not as good looking as the t2i pipeline which I later figured out which is actually a big flaw. The new system is as follows: We get the latent from encoder. We also get timesteps from the scheduler. We decide on a  percentage of available steps that we want to go over which essentially allows us to decide the randomness level from the image guidance perspective. We get a parameter named "creativity" which takes values in range [0,1]. You can find example images of it at examples/alper-4_12_2025_v2


- 4.12.2025 4PM

I was able to integrate i2i and t2i pipelines from ti2v task. For t2i task was actually trivial because I only updated # of frames to be generated and save the resulting 
tensor as image which util was already implemented

i2i task was a little bit tricky because ti2v pipeline starts from the given image and gradually get a scene out of it. However in order to generate a single image, the idea of generating a series of images to wait that kind
of variance does not make sense. When I looked at the code and look at the paper, I see that latenst(image) are initialized as random in t2v pipeline and with the image encoding in i2v pipeline. We need something in the middle in order to 
generate a different image compared to given image. 

When I look at the code they use masks to either mask out the image latent or random noise. Because we need to generate a latent which is not too far away from the given image's latent but still be close to it
I decided to use mask values which range between 0-1 rather than using fixed 1's or 0's. And set it as a hyperparameter named "creativity". This way we can tune this parameter to decide on how different we want new image to be from the given.

I have given examples in the folder: examples/alper-4_12_2025

!! Observation: The generated image in the i2i pipeline looks suboptimal with visible texture problems. Is this related to wrong configuration of latent variable update ? I' ll be looking into this in more detail.






- 3.12.2025 11PM

I have skimmed trough the paper. It seems that they have pretrained the model first with text-to-image dataset ensuring the cross-model semantic-textual alignment. 
This gives a huge hint about text-to-image implementation. 

Also when I look at the model architecture, I see an MLP block used for integrating timestep information. We do not need this block since we will not be generating videos. We can record the outputs for 0th timestep and use that value as input to the relevant transformer blocks

- 3.12.2025

I got the description of implementing Wan2.2 t2i and i2i. However, Wan2.2 is a video generation model. Using wan2.2 api to generate a single frame video is trivial which should be bare minimum(implementing requirements for serverless). 
I believe going over the architecture of Wan2.2 I can isolate the process of generating single image from the existing functionality at the model. Therefore I start working by reading the paper of Wan2.2

