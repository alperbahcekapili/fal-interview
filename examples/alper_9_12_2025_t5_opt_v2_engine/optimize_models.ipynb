{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63bfa6b",
   "metadata": {},
   "source": [
    "In this notebook I will try to convert the models into tensorrt engines by firs converting them into onnx counterparts then using trt_exec cli we will optimize the computation graph' s for our specific hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c3fbf",
   "metadata": {},
   "source": [
    "Let us start by our t5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6855434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpfischer/miniconda3/envs/wan310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "TensorRT-LLM is not installed. Please install TensorRT-LLM or set TRTLLM_PLUGINS_PATH to the directory containing libnvinfer_plugin_tensorrt_llm.so to use converters for torch.distributed ops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model to cpu first\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_tensorrt\n",
    "\n",
    "from wan.utils.timer import Timer\n",
    "\n",
    "from wan.modules.t5 import T5EncoderModel\n",
    "\n",
    "# wan_shared_cfg.t5_model = 'umt5_xxl'\n",
    "# wan_shared_cfg.t5_dtype = torch.bfloat16\n",
    "# wan_shared_cfg.text_len = 512\n",
    "# ti2v_5B.t5_checkpoint = 'models_t5_umt5-xxl-enc-bf16.pth'\n",
    "# ti2v_5B.t5_tokenizer = 'google/umt5-xxl'\n",
    "\n",
    "\n",
    "print(\"Loading model to cpu first\")\n",
    "model = T5EncoderModel(\n",
    "    text_len=512, # from config\n",
    "    dtype=torch.float16,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    checkpoint_path=\"Wan2.2-TI2V-5B/models_t5_umt5-xxl-enc-bf16.pth\",\n",
    "    tokenizer_path=\"Wan2.2-TI2V-5B/google/umt5-xxl\"\n",
    "\n",
    ")\n",
    "# print(\"Transfering model to cuda\")\n",
    "# model.model.eval().to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "texts = [\"Alper example input\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c31298f",
   "metadata": {},
   "source": [
    "Tokenization logic from T5EncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188a558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "to_optimize = model.model\n",
    "ids, mask = model.tokenizer(\n",
    "    texts, return_mask=True, add_special_tokens=True)\n",
    "ids = ids.to(device).to(torch.int32)\n",
    "mask = mask.to(device).to(torch.int32)\n",
    "seq_lens = mask.gt(0).sum(dim=1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf477a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,           \n",
    "    (ids, mask),\n",
    "    \"t5model.onnx\",\n",
    "    opset_version=17,           \n",
    "    input_names=['input'],      \n",
    "    output_names=['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8818c",
   "metadata": {},
   "source": [
    "I save the relevant tensors to disk in order not to generate the over and over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfd8876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ids, open(\"ids.pth\", \"wb\"))\n",
    "torch.save(mask, open(\"mask.pth\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54faf540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ids = torch.load(open(\"ids.pth\", \"rb\"))\n",
    "mask = torch.load(open(\"mask.pth\", \"rb\"))\n",
    "ids_device = ids.cuda().to(torch.long).contiguous()\n",
    "mask_device = mask.cuda().to(torch.long).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b7e71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading engine from /home/alpfischer/Wan2.2/Wan2.2-TI2V-5B/t5_model_last.engine...\n",
      "\n",
      "INSPECTING TENSORS:\n",
      "  Input found: input\n",
      "trt_dtype for input:DataType.INT64\n",
      "  Input found: onnx::Reshape_1\n",
      "trt_dtype for onnx::Reshape_1:DataType.INT64\n",
      "  Output found: output\n",
      "trt_dtype for output:DataType.BF16\n",
      "\n",
      "Generating dummy input data...\n",
      "Running Inference...\n",
      "Did execution request succeed? True\n",
      "Inference Complete.\n",
      "Output Shape: (2097152,)\n",
      "Output Data snippet: [ 15029 -17462 -17242  15968 -17255 -17593  15572  15915 -16869 -16961]\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "# 1. Setup Logger and Load Engine\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "# engine_path = \"/home/alpfischer/Wan2.2/t5_model_last.engine\"\n",
    "engine_path = \"/home/alpfischer/Wan2.2/Wan2.2-TI2V-5B/t5_model_last.engine\"\n",
    "\n",
    "\n",
    "print(f\"Loading engine from {engine_path}...\")\n",
    "with open(engine_path, \"rb\") as f:\n",
    "    engine_data = f.read()\n",
    "\n",
    "runtime = trt.Runtime(TRT_LOGGER)\n",
    "engine = runtime.deserialize_cuda_engine(engine_data)\n",
    "\n",
    "if engine is None:\n",
    "    raise RuntimeError(\"Failed to load engine!\")\n",
    "\n",
    "# 2. Create Context and Stream\n",
    "context = engine.create_execution_context()\n",
    "stream = cuda.Stream()\n",
    "\n",
    "# 3. Define Input Shapes\n",
    "# You stated inputs are (1, 512). T5 usually takes INT32 for input_ids.\n",
    "input_shape = (1, 512) \n",
    "input_dtype = np.int64 \n",
    "\n",
    "# Prepare dictionaries to hold memory pointers\n",
    "host_inputs = []\n",
    "cuda_inputs = []\n",
    "host_outputs = []\n",
    "cuda_outputs = []\n",
    "bindings = [] # For legacy viewing, though we use set_tensor_address\n",
    "\n",
    "print(\"\\nINSPECTING TENSORS:\")\n",
    "# 4. Allocate Memory Helper\n",
    "for i in range(engine.num_io_tensors):\n",
    "    tensor_name = engine.get_tensor_name(i)\n",
    "    tensor_mode = engine.get_tensor_mode(tensor_name)\n",
    "    \n",
    "    # Check if it is input or output\n",
    "    if tensor_mode == trt.TensorIOMode.INPUT:\n",
    "        print(f\"  Input found: {tensor_name}\")\n",
    "        # Set the shape for the specific input\n",
    "        context.set_input_shape(tensor_name, input_shape)\n",
    "    else:\n",
    "        print(f\"  Output found: {tensor_name}\")\n",
    "\n",
    "    # Determine required memory size\n",
    "    # Note: We query the context for shape because output sizes might be dynamic based on input\n",
    "    shape = context.get_tensor_shape(tensor_name)\n",
    "\n",
    "\n",
    "    # Get the TensorRT type\n",
    "    trt_dtype = engine.get_tensor_dtype(tensor_name)\n",
    "\n",
    "    print(f\"trt_dtype for {tensor_name}:{trt_dtype}\")\n",
    "\n",
    "    # manual mapping for types that trt.nptype() misses\n",
    "    if trt_dtype == trt.DataType.BF16:\n",
    "        # NumPy doesn't have native BF16, so we usually map it to int16 for storage \n",
    "        # or float32 if you plan to convert immediately. \n",
    "        # For pure buffer size calculation, int16 (2 bytes) is the correct size equivalent.\n",
    "        dtype = np.dtype(np.int16) \n",
    "    elif trt_dtype == trt.DataType.FP8:\n",
    "        # FP8 takes 1 byte, similar to int8/uint8\n",
    "        dtype = np.dtype(np.int8)\n",
    "    else:\n",
    "        # Fallback to the standard mapping for Float32, Float16, Int32, etc.\n",
    "        try:\n",
    "            dtype = np.dtype(trt.nptype(trt_dtype))\n",
    "        except TypeError:\n",
    "            # If it still fails, print the type to debug\n",
    "            print(f\"Error: Unsupported TensorRT type found: {trt_dtype}\")\n",
    "            raise\n",
    "\n",
    "    # Calculate volume (product of dimensions)\n",
    "    vol = 1\n",
    "    for dim in shape:\n",
    "        # Safety check for dynamic shapes that weren't resolved\n",
    "        if dim < 0: \n",
    "            raise RuntimeError(f\"Found dynamic shape dimension {dim} for {tensor_name}. Ensure input shapes are set correctly.\")\n",
    "        vol *= dim\n",
    "            \n",
    "    size = vol * dtype.itemsize\n",
    " \n",
    "    # Allocate Device Memory\n",
    "    d_mem = cuda.mem_alloc(size)\n",
    "    \n",
    "    # Allocation Host Memory (Page-locked/Pinned for speed)\n",
    "    h_mem = cuda.pagelocked_empty(vol, dtype)\n",
    "    \n",
    "    # Store pointers and bind address\n",
    "    # IMPORTANT: TensorRT > 8.5 uses set_tensor_address\n",
    "    context.set_tensor_address(tensor_name, int(d_mem))\n",
    "\n",
    "    if tensor_mode == trt.TensorIOMode.INPUT:\n",
    "        host_inputs.append(h_mem)\n",
    "        cuda_inputs.append(d_mem)\n",
    "    else:\n",
    "        host_outputs.append(h_mem)\n",
    "        cuda_outputs.append(d_mem)\n",
    "\n",
    "# 5. Prepare Dummy Data (Replace this with your actual T5 Token IDs)\n",
    "# T5 Inputs: Usually [input_ids, decoder_input_ids]\n",
    "print(\"\\nGenerating dummy input data...\")\n",
    "np.copyto(host_inputs[0], ids.detach().numpy().astype(input_dtype))\n",
    "np.copyto(host_inputs[1], mask.detach().numpy().astype(input_dtype))\n",
    "\n",
    "\n",
    "# 6. Inference Loop\n",
    "print(\"Running Inference...\")\n",
    "\n",
    "# Copy Host -> Device\n",
    "for h_mem, d_mem in zip(host_inputs, cuda_inputs):\n",
    "    cuda.memcpy_htod_async(d_mem, h_mem, stream)\n",
    "\n",
    "# Execute (Async v3 is the standard for modern TRT)\n",
    "success = context.execute_async_v3(stream_handle=stream.handle)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(f\"Did execution request succeed? {success}\")\n",
    "\n",
    "\n",
    "# Copy Device -> Host\n",
    "for h_mem, d_mem in zip(host_outputs, cuda_outputs):\n",
    "    cuda.memcpy_dtoh_async(h_mem, d_mem, stream)\n",
    "\n",
    "# Synchronize to ensure completion\n",
    "stream.synchronize()\n",
    "\n",
    "print(\"Inference Complete.\")\n",
    "print(f\"Output Shape: {host_outputs[0].shape}\")\n",
    "print(\"Output Data snippet:\", host_outputs[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa8cc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_name = engine.get_tensor_name(2)\n",
    "shape = context.get_tensor_shape(tensor_name)\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd8f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3221d11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(host_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e00044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded values: [ 0.  0. -0. ...  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ml_dtypes\n",
    "\n",
    "# 1. Assume 'trt_output' is your int16 array from TensorRT\n",
    "#    e.g., trt_output = np.array([16256, 16384], dtype=np.int16) \n",
    "\n",
    "# 2. View the int16 bits as bfloat16\n",
    "bf16_data = host_outputs[0].view(ml_dtypes.bfloat16)\n",
    "\n",
    "# 3. Convert to float32 for comparison/math\n",
    "final_output = bf16_data.astype(np.float32)\n",
    "\n",
    "print(\"Decoded values:\", final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f17bdce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = mask.gt(0).sum(dim=1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3398abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = torch.Tensor(final_output).reshape([1,512,4096])\n",
    "t5_engine_output = [u[:v] for u, v in zip( final_output, seq_lens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36a64c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 4096])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7a10809",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"t5_engine_outputs.pth\", \"wb\") as f:\n",
    "    torch.save(torch.Tensor(final_output), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d9c5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.model.eval().to(\"cuda\")\n",
    "pytorch_model_output = model(texts, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba8c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"t5_engine_outputs.pth\", \"rb\") as f:\n",
    "    engine_output = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76db747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 4096])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54e59cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model_output[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba9ec76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0014, -0.0062, -0.0203,  ...,  0.0002,  0.1084, -0.1260],\n",
       "         [ 0.0030,  0.0118, -0.0171,  ...,  0.0014,  0.0227, -0.0204],\n",
       "         [ 0.0016,  0.0204,  0.0635,  ...,  0.0008, -0.0471, -0.1504],\n",
       "         ...,\n",
       "         [ 0.0009, -0.0082,  0.0596,  ...,  0.0002,  0.0461,  0.0265],\n",
       "         [ 0.0009, -0.0082,  0.0596,  ...,  0.0002,  0.0461,  0.0265],\n",
       "         [ 0.0009, -0.0082,  0.0596,  ...,  0.0002,  0.0461,  0.0265]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eca584d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0014, -0.0061, -0.0205,  ...,  0.0002,  0.1085, -0.1257],\n",
       "        [ 0.0030,  0.0117, -0.0172,  ...,  0.0014,  0.0228, -0.0203],\n",
       "        [ 0.0016,  0.0202,  0.0638,  ...,  0.0008, -0.0472, -0.1509],\n",
       "        [ 0.0014,  0.0591,  0.0010,  ...,  0.0005, -0.0332, -0.0512],\n",
       "        [-0.0011, -0.0063,  0.0008,  ...,  0.0003, -0.0091,  0.0115]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e63aec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(engine_output[0][:5].to(\"cpu\")-pytorch_model_output[0].to(\"cpu\")).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c5b3ae",
   "metadata": {},
   "source": [
    "As we can see diff is very small that means our engine export from trtexec is valid. Let's move to the implementation and utilize the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c79b6409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3f2bae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2080d2",
   "metadata": {},
   "source": [
    "After integrating tensorrt engine into our pipeline we see that single inference with trt engine reduce the total inference time of a single t5 encoder from 0.1 ( which was obtained via negative prompt caching and torch.compile) seconds to 0.009 seconds which is a huge speed gain of %50 means we can process x10 more text in one go. We can apply the same operation to VAE and Diffusion models as well. Because this demonstration of huge speed gain can be easily extended to other models I am skipping doing the same for them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43498b67",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wan310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
